{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Maximizing conversion rates and profit of an e-shop company with additional physical stores\n",
    "\n",
    "## Business Challenge\n",
    "\n",
    "The company is sending promotional offers / incentives to its customers. They are sending them either via personalized emails or generic app notifications. Due to time capacity of the staff only 30% of the offers / incentives are sent via personalized emails. The remaining 70% of the offers / incentives are handled by automated generic e-shop app notifications. Currently, the selection of the customers who receive their offers / incentives via personalized emails or generic app notifications is random.\n",
    "The management team has observed that customers who receive offers / incentives via personalized emails have a higher conversion rate compared to those who are sent offers / incentives via generic app notifications. However, they also recognize that not all customers or products yield the same profits or have the same likelihood of conversion.\n",
    "The company wants to optimize the selection of the customers who will receive their offers / incentives via personalized emails and who will receive their offers / incentives via generic app notifications in order to maximize profits.\n",
    "\n",
    "## Available Data\n",
    "\n",
    "- Customer_ID: unique id representing a customer\n",
    "- Chain: integer representing a store chain\n",
    "- Gender: gender of the customer\n",
    "- Offer_ID: id representing a certain offer\n",
    "- Market: id representing a geographical region\n",
    "- Previous_Interactions: number of previous interactions with the company\n",
    "- Brand: brand of the product\n",
    "- Sales_Price: price of the product\n",
    "- Purchase_Price: acquisition price of the product for which the company has bought the item\n",
    "- Sales_Expense: additional expenses associated with the sale (e.g., shipping, handling)\n",
    "- Offer_Incentive_Sent_Via: channel through which the offer was sent (personalized email or generic app notification)\n",
    "- Channel: channel through which the customer interacted with the company while asking additional questions about the order\n",
    "- Conversion: whether the customer bought the product or not\n",
    "- Preferred_Channel: channel through which the customer prefers to interact with the company while asking additional questions about the order\n",
    "- Profit: profit made from the sale (= Sales_Price - Purchase_Price - Sales_Expense)\n",
    "- Competitor_Price: price of the product from a competitor\n",
    "- Price_Difference: difference between the price of the product and the price of the same product from a competitor\n",
    "\n",
    "## Bonus Tasks\n",
    "\n",
    "This notebook is missing a couple of methodological implementations compared to the lecture notebook, so the first bonus task is to implement them utilizing the OOP best practices. Moreover, the ModelTrainer and ModelCalibrationTrainer classes are intentionally written in a way that one of them is redundant, so the second bonus task is to come up with and implement a solution to combine them into one class without losing the flexibility of the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import threading\n",
    "import joblib\n",
    "import logging\n",
    "import warnings\n",
    "from typing import Dict, List, Tuple, Any, Callable, Optional, TypedDict, ClassVar\n",
    "from abc import ABC, abstractmethod\n",
    "from dataclasses import dataclass\n",
    "import time\n",
    "from functools import wraps\n",
    "\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.metrics import classification_report, roc_auc_score, brier_score_loss\n",
    "from sklearn.calibration import CalibrationDisplay, CalibratedClassifierCV\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOGGING_CONFIG = {\n",
    "    \"version\": 1,\n",
    "    \"disable_existing_loggers\": False,\n",
    "    \"formatters\": {\n",
    "        \"standard\": {\"format\": \"%(asctime)s - %(name)s - %(levelname)s - %(message)s\"},\n",
    "        \"detailed\": {\n",
    "            \"format\": \"%(asctime)s - %(name)s - %(levelname)s - [%(filename)s:%(lineno)d] - %(message)s\"\n",
    "        },\n",
    "    },\n",
    "    \"handlers\": {\n",
    "        \"console\": {\n",
    "            \"level\": \"INFO\",\n",
    "            \"formatter\": \"standard\",\n",
    "            \"class\": \"logging.StreamHandler\",\n",
    "        },\n",
    "        \"file\": {\n",
    "            \"level\": \"DEBUG\",\n",
    "            \"formatter\": \"detailed\",\n",
    "            \"class\": \"logging.FileHandler\",\n",
    "            \"filename\": \"app.log\",\n",
    "            \"mode\": \"a\",\n",
    "        },\n",
    "    },\n",
    "    \"loggers\": {\n",
    "        \"\": {\"handlers\": [\"console\", \"file\"], \"level\": \"INFO\", \"propagate\": False},\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.config.dictConfig(LOGGING_CONFIG)\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "pd.set_option(\"display.max_colwidth\", 500)\n",
    "pd.set_option(\"display.float_format\", lambda x: \"%.5f\" % x)\n",
    "random.seed(hash(\"abc\") % 2**32 - 1)\n",
    "np.random.seed(hash(\"xyz\") % 2**32 - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BlockTimer:\n",
    "    \"\"\"\n",
    "    A context manager class for timing code blocks.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, name: str):\n",
    "        self.name = name\n",
    "        self.start_time: Optional[float] = None\n",
    "\n",
    "    def __enter__(self):\n",
    "        self.start_time = time.time()\n",
    "        logging.info(f\"Starting {self.name}\")\n",
    "        return self\n",
    "\n",
    "    def __exit__(self, exc_type, exc_value, traceback):\n",
    "        elapsed_time = time.time() - self.start_time\n",
    "        logging.info(f\"{self.name} - elapsed time: {elapsed_time:.2f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def method_timer(method: Callable[..., Any]) -> Callable[..., Any]:\n",
    "    \"\"\"\n",
    "    Decorator to measure the execution time of methods.\n",
    "    \"\"\"\n",
    "\n",
    "    @wraps(method)\n",
    "    def wrapper(self: Any, *args: Any, **kwargs: Any) -> Any:\n",
    "        start_time = time.time()\n",
    "        result = method(self, *args, **kwargs)\n",
    "        end_time = time.time()\n",
    "        elapsed_time = end_time - start_time\n",
    "        logger = logging.getLogger(self.__class__.__name__)\n",
    "        logger.info(f\"Method {method.__name__} executed in {elapsed_time:.2f}s\")\n",
    "        return result\n",
    "\n",
    "    return wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_method_call(method: Callable[..., Any]) -> Callable[..., Any]:\n",
    "    \"\"\"\n",
    "    Decorator to log method calls.\n",
    "    \"\"\"\n",
    "\n",
    "    @wraps(method)\n",
    "    def wrapper(self: Any, *args: Any, **kwargs: Any) -> Any:\n",
    "        logger = logging.getLogger(self.__class__.__name__)\n",
    "        logger.info(f\"Called method {method.__name__}\")\n",
    "        return method(self, *args, **kwargs)\n",
    "\n",
    "    return wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SingletonMeta(type):\n",
    "    \"\"\"\n",
    "    Thread-safe implementation of Singleton.\n",
    "    \"\"\"\n",
    "\n",
    "    _instance: Optional[\"SingletonMeta\"] = None\n",
    "    _lock: threading.Lock = threading.Lock()\n",
    "\n",
    "    def __call__(cls, *args, **kwargs):\n",
    "        with cls._lock:\n",
    "            if cls._instance is None:\n",
    "                cls._instance = super().__call__(*args, **kwargs)\n",
    "        return cls._instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataProcessingError(Exception):\n",
    "    \"\"\"\n",
    "    Exception raised for errors in the data processing pipeline.\n",
    "    \"\"\"\n",
    "\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelTrainingError(Exception):\n",
    "    \"\"\"\n",
    "    Exception raised for errors during model training.\n",
    "    \"\"\"\n",
    "\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelEvaluationError(Exception):\n",
    "    \"\"\"\n",
    "    Exception raised for errors during model training.\n",
    "    \"\"\"\n",
    "\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(*, file_path: str, version: str = None) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Load data from a CSV file, optionally loading a specific version.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): Path to the data file.\n",
    "        version (str, optional): Specific version identifier of the data.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Loaded data.\n",
    "    \"\"\"\n",
    "    if version:\n",
    "        file_path = f\"{file_path}_v{version}.csv\"\n",
    "    else:\n",
    "        file_path = f\"{file_path}.csv\"\n",
    "    try:\n",
    "        data = pd.read_csv(file_path)\n",
    "        logging.info(f\"Data loaded from {file_path}\")\n",
    "        return data\n",
    "    except FileNotFoundError as e:\n",
    "        logging.error(f\"Data file not found: {file_path}\")\n",
    "        raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_data(file_path=\"dataset\", version=\"5\")\n",
    "data[\"Price_Difference\"] = data[\"Sales_Price\"] - data[\"Competitor_Price\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConversionRateAnalyzer:\n",
    "    \"\"\"\n",
    "    A class to analyze and visualize the conversion rates and profits.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, data: pd.DataFrame) -> None:\n",
    "        \"\"\"\n",
    "        Initialize the DataAnalyzer with the dataset.\n",
    "\n",
    "        Args:\n",
    "            data (pd.DataFrame): The dataset to analyze.\n",
    "        \"\"\"\n",
    "        self.data = data\n",
    "        self.logger = logging.getLogger(self.__class__.__name__)\n",
    "        return None\n",
    "\n",
    "    @log_method_call\n",
    "    def analyze_conversion_rates(self) -> None:\n",
    "        \"\"\"\n",
    "        Analyze and visualize conversion rates by Offer_Incentive_Sent_Via.\n",
    "        \"\"\"\n",
    "        conversion_rates = self.data.groupby(\"Offer_Incentive_Sent_Via\")[\n",
    "            \"Conversion\"\n",
    "        ].mean()\n",
    "        print(\"Conversion Rates by Offer_Incentive_Sent_Via:\")\n",
    "        print(conversion_rates)\n",
    "\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        sns.barplot(x=conversion_rates.index, y=conversion_rates.values)\n",
    "        plt.title(\"Conversion Rates by Offer_Incentive_Sent_Via\")\n",
    "        plt.ylabel(\"Conversion Rate\")\n",
    "        plt.show()\n",
    "        return None\n",
    "\n",
    "    @log_method_call\n",
    "    def analyze_profit_by_response(self) -> None:\n",
    "        \"\"\"\n",
    "        Analyze and visualize total profit by Offer_Incentive_Sent_Via.\n",
    "        \"\"\"\n",
    "        profit_by_response = self.data.groupby(\"Offer_Incentive_Sent_Via\")[\n",
    "            \"Profit\"\n",
    "        ].sum()\n",
    "        print(\"\\nTotal Profit by Offer_Incentive_Sent_Via:\")\n",
    "        print(profit_by_response)\n",
    "\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        sns.barplot(x=profit_by_response.index, y=profit_by_response.values)\n",
    "        plt.title(\"Total Profit by Offer_Incentive_Sent_Via\")\n",
    "        plt.ylabel(\"Total Profit\")\n",
    "        plt.show()\n",
    "        return None\n",
    "\n",
    "    @log_method_call\n",
    "    def analyze_product_conversion_rates(self) -> None:\n",
    "        \"\"\"\n",
    "        Analyze and visualize conversion rates by Offer_Incentive_Sent_Via and Brand.\n",
    "        \"\"\"\n",
    "        product_conversion_rates = (\n",
    "            self.data.groupby([\"Offer_Incentive_Sent_Via\", \"Brand\"])[\"Conversion\"]\n",
    "            .mean()\n",
    "            .unstack()\n",
    "        )\n",
    "        print(\"\\nConversion Rates by Offer_Incentive_Sent_Via and Brand:\")\n",
    "        print(product_conversion_rates)\n",
    "\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        sns.heatmap(product_conversion_rates, annot=True, cmap=\"YlGnBu\", fmt=\".2f\")\n",
    "        plt.title(\"Conversion Rates by Offer_Incentive_Sent_Via and Brand\")\n",
    "        plt.show()\n",
    "        return None\n",
    "\n",
    "    @log_method_call\n",
    "    def analyze_income_conversion_rates(self) -> None:\n",
    "        \"\"\"\n",
    "        Analyze and visualize conversion rates by Offer_Incentive_Sent_Via and Offer_ID.\n",
    "        \"\"\"\n",
    "        income_conversion_rates = (\n",
    "            self.data.groupby([\"Offer_Incentive_Sent_Via\", \"Offer_ID\"])[\"Conversion\"]\n",
    "            .mean()\n",
    "            .unstack()\n",
    "        )\n",
    "        print(\"\\nConversion Rates by Offer_Incentive_Sent_Via and Offer_ID:\")\n",
    "        print(income_conversion_rates)\n",
    "\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        sns.heatmap(income_conversion_rates, annot=True, cmap=\"YlGnBu\", fmt=\".2f\")\n",
    "        plt.title(\"Conversion Rates by Offer_Incentive_Sent_Via and Offer_ID\")\n",
    "        plt.show()\n",
    "        return None\n",
    "\n",
    "    @log_method_call\n",
    "    def run_all(self) -> None:\n",
    "        \"\"\"\n",
    "        Run all analysis methods in sequence.\n",
    "        \"\"\"\n",
    "        self.analyze_conversion_rates()\n",
    "        self.analyze_profit_by_response()\n",
    "        self.analyze_product_conversion_rates()\n",
    "        self.analyze_income_conversion_rates()\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyzer = ConversionRateAnalyzer(data)\n",
    "analyzer.run_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass(frozen=True)\n",
    "class DataPreprocessorConfig(metaclass=SingletonMeta):\n",
    "    \"\"\"\n",
    "    Configuration class for DataPreprocessor.\n",
    "    \"\"\"\n",
    "\n",
    "    FEATURES: ClassVar[List[str]] = [\n",
    "        \"Chain\",\n",
    "        \"Gender\",\n",
    "        \"Offer_ID\",\n",
    "        \"Market\",\n",
    "        \"Previous_Interactions\",\n",
    "        \"Brand\",\n",
    "        \"Sales_Price\",\n",
    "        \"Channel\",\n",
    "        \"Price_Difference\",\n",
    "        \"Competitor_Price\",\n",
    "    ]\n",
    "    CATEGORICAL_FEATURES: ClassVar[List[str]] = [\n",
    "        \"Gender\",\n",
    "        \"Offer_ID\",\n",
    "        \"Market\",\n",
    "        \"Brand\",\n",
    "        \"Channel\",\n",
    "        \"Preferred_Channel\",\n",
    "        \"Offer_Incentive_Sent_Via\",\n",
    "    ]\n",
    "    NUMERICAL_FEATURES: ClassVar[List[str]] = [\n",
    "        \"Chain\",\n",
    "        \"Previous_Interactions\",\n",
    "        \"Sales_Price\",\n",
    "        \"Price_Difference\",\n",
    "        \"Competitor_Price\",\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataPreprocessor:\n",
    "    \"\"\"\n",
    "    A class to preprocess the dataset.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, data: pd.DataFrame, resample: bool = True) -> None:\n",
    "        \"\"\"\n",
    "        Initialize the DataPreprocessor with the dataset.\n",
    "\n",
    "        Args:\n",
    "            data (pd.DataFrame): The dataset to preprocess.\n",
    "        \"\"\"\n",
    "        self.data = data\n",
    "        self.features = DataPreprocessorConfig.FEATURES\n",
    "        self.categorical_features = DataPreprocessorConfig.CATEGORICAL_FEATURES\n",
    "        self.numerical_features = DataPreprocessorConfig.NUMERICAL_FEATURES\n",
    "        self.resample = resample\n",
    "        self.logger = logging.getLogger(self.__class__.__name__)\n",
    "        self._X: Optional[pd.DataFrame] = None\n",
    "        self._y: Optional[pd.Series] = None\n",
    "        self._cache: Dict[str, Any] = {}\n",
    "        return None\n",
    "\n",
    "    @log_method_call\n",
    "    def encode_categorical_features(self) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Encode categorical features using one-hot encoding.\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: The dataset with encoded categorical features.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            encoded_data = pd.get_dummies(\n",
    "                self.data, columns=self.categorical_features, drop_first=True\n",
    "            )\n",
    "            return encoded_data\n",
    "        except Exception as e:\n",
    "            self.logger.error(\"Error encoding categorical features\")\n",
    "            raise DataProcessingError(e) from e\n",
    "\n",
    "    @log_method_call\n",
    "    def get_feature_columns(self, data_encoded: pd.DataFrame) -> List[str]:\n",
    "        \"\"\"\n",
    "        Get the list of feature columns, including dummy variables.\n",
    "\n",
    "        Args:\n",
    "            data_encoded (pd.DataFrame): The encoded dataset.\n",
    "\n",
    "        Returns:\n",
    "            List[str]: List of feature column names.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            dummy_columns = [\n",
    "                col\n",
    "                for col in data_encoded.columns\n",
    "                if any(cat in col for cat in self.categorical_features)\n",
    "            ]\n",
    "            return self.numerical_features + dummy_columns\n",
    "        except Exception as e:\n",
    "            self.logger.error(\"Error getting feature columns\")\n",
    "            raise DataProcessingError(e) from e\n",
    "\n",
    "    @log_method_call\n",
    "    def prepare_data(self) -> Tuple[pd.DataFrame, pd.Series]:\n",
    "        \"\"\"\n",
    "        Prepare the data by encoding categorical features\n",
    "        and selecting relevant columns.\n",
    "\n",
    "        Returns:\n",
    "            Tuple[pd.DataFrame, pd.Series]: Features (X) and target variable (y).\n",
    "        \"\"\"\n",
    "        try:\n",
    "            cache_key = \"prepare_data\"\n",
    "            if cache_key in self._cache:\n",
    "                self.logger.info(\"Using cached processed data.\")\n",
    "                return self._cache[cache_key]\n",
    "\n",
    "            data_encoded = self.encode_categorical_features()\n",
    "            feature_columns = self.get_feature_columns(data_encoded)\n",
    "            X = data_encoded[feature_columns]\n",
    "            y = data_encoded[\"Conversion\"]\n",
    "\n",
    "            self._cache[cache_key] = (X, y)\n",
    "            return X, y\n",
    "        except Exception as e:\n",
    "            self.logger.error(\"Error preparing data\")\n",
    "            raise DataProcessingError(e) from e\n",
    "\n",
    "    @log_method_call\n",
    "    def split_data(\n",
    "        self, test_size: float = 0.3, random_state: int = 42\n",
    "    ) -> Tuple[pd.DataFrame, pd.DataFrame, pd.Series, pd.Series]:\n",
    "        \"\"\"\n",
    "        Split the data into training and testing sets.\n",
    "\n",
    "        Args:\n",
    "            test_size (float): Proportion of the dataset to include in the test split.\n",
    "            random_state (int): Random state for reproducibility.\n",
    "\n",
    "        Returns:\n",
    "            Tuple[pd.DataFrame, pd.DataFrame, pd.Series, pd.Series]: X_train, X_test, y_train, y_test\n",
    "        \"\"\"\n",
    "        try:\n",
    "            X, y = self.prepare_data()\n",
    "            if self.resample:\n",
    "                X_train, X_test, y_train, y_test = train_test_split(\n",
    "                    X, y, test_size=test_size, stratify=y, random_state=random_state\n",
    "                )\n",
    "                train_data = X_train.copy()\n",
    "                train_data[\"Conversion\"] = y_train\n",
    "                minority_class = train_data[train_data[\"Conversion\"] == 1]\n",
    "                majority_class = train_data[train_data[\"Conversion\"] == 0]\n",
    "                minority_cnt = len(minority_class)\n",
    "                self.logger.info(f\"Minority class count: {minority_cnt}\")\n",
    "                majority_class_sampled = majority_class.sample(\n",
    "                    n=minority_cnt, random_state=random_state\n",
    "                )\n",
    "                balanced_train_data = pd.concat(\n",
    "                    [minority_class, majority_class_sampled]\n",
    "                )\n",
    "                balanced_train_data = balanced_train_data.sample(\n",
    "                    frac=1, random_state=random_state\n",
    "                ).reset_index(drop=True)\n",
    "                X_train = balanced_train_data.drop(\"Conversion\", axis=1)\n",
    "                y_train = balanced_train_data[\"Conversion\"]\n",
    "            else:\n",
    "                X_train, X_test, y_train, y_test = train_test_split(\n",
    "                    X, y, test_size=test_size, stratify=y, random_state=random_state\n",
    "                )\n",
    "            return X_train, X_test, y_train, y_test\n",
    "        except Exception as e:\n",
    "            self.logger.error(\"Error splitting data\")\n",
    "            raise DataProcessingError(e) from e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = DataPreprocessor(data, resample=True)\n",
    "X_train, X_test, y_train, y_test = preprocessor.split_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseTrainer(ABC):\n",
    "    \"\"\"\n",
    "    Abstract base class for model training.\n",
    "    \"\"\"\n",
    "\n",
    "    @abstractmethod\n",
    "    def train_models(self):\n",
    "        \"\"\"\n",
    "        Train models.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def _evaluate_model(self, model: BaseEstimator) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Evaluate a trained model.\n",
    "        \"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelEvaluationResult(TypedDict):\n",
    "    \"\"\"\n",
    "    Result of model evaluation.\n",
    "    \"\"\"\n",
    "\n",
    "    classification_report: str\n",
    "    roc_auc_score: float\n",
    "    brier_score: float\n",
    "    expected_profit: float\n",
    "    y_pred: np.ndarray\n",
    "    y_pred_proba: np.ndarray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelTrainer(BaseTrainer):\n",
    "    \"\"\"\n",
    "    A class to train and evaluate multiple classification models.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        X_train: pd.DataFrame,\n",
    "        X_test: pd.DataFrame,\n",
    "        y_train: pd.Series,\n",
    "        y_test: pd.Series,\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Initialize the ModelTrainer with training and testing data.\n",
    "\n",
    "        Args:\n",
    "            X_train (pd.DataFrame): Training features\n",
    "            X_test (pd.DataFrame): Testing features\n",
    "            y_train (pd.Series): Training labels\n",
    "            y_test (pd.Series): Testing labels\n",
    "        \"\"\"\n",
    "        self.X_train = X_train\n",
    "        self.X_test = X_test\n",
    "        self.y_train = y_train\n",
    "        self.y_test = y_test\n",
    "        self.models = {}\n",
    "        self.results = {}\n",
    "        self.logger = logging.getLogger(self.__class__.__name__)\n",
    "        return None\n",
    "\n",
    "    @staticmethod\n",
    "    def initialize_models(random_state: int = 42) -> Dict[str, BaseEstimator]:\n",
    "        \"\"\"\n",
    "        Initialize models to be trained.\n",
    "\n",
    "        Args:\n",
    "            random_state (int): Random state for reproducibility\n",
    "\n",
    "        Returns:\n",
    "            Dict[str, BaseEstimator]: Dictionary of model instances\n",
    "        \"\"\"\n",
    "        return {\n",
    "            \"RandomForest\": RandomForestClassifier(random_state=random_state),\n",
    "            \"XGBoost\": XGBClassifier(\n",
    "                random_state=random_state,\n",
    "                eval_metric=\"logloss\",\n",
    "            ),\n",
    "        }\n",
    "\n",
    "    @staticmethod\n",
    "    def get_param_distributions() -> Dict[str, Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        Define hyperparameter search spaces for each model.\n",
    "\n",
    "        Returns:\n",
    "            Dict[str, Dict[str, Any]]: Dictionary of parameter distributions for each model\n",
    "        \"\"\"\n",
    "        return {\n",
    "            \"RandomForest\": {\n",
    "                \"n_estimators\": [100, 200, 300, 400, 500],\n",
    "                \"max_depth\": [None, 10, 20, 30, 40, 50],\n",
    "                \"min_samples_split\": [2, 5, 10],\n",
    "                \"min_samples_leaf\": [1, 2, 4],\n",
    "                \"max_features\": [\"sqrt\", \"log2\", None],\n",
    "                \"class_weight\": [\"balanced\", \"balanced_subsample\", None],\n",
    "            },\n",
    "            \"XGBoost\": {\n",
    "                \"n_estimators\": [100, 200, 300, 400, 500],\n",
    "                \"max_depth\": [3, 4, 5, 6, 7, 8],\n",
    "                \"learning_rate\": [0.01, 0.05, 0.1, 0.15, 0.2],\n",
    "                \"subsample\": [0.6, 0.7, 0.8, 0.9, 1.0],\n",
    "                \"colsample_bytree\": [0.6, 0.7, 0.8, 0.9, 1.0],\n",
    "                \"min_child_weight\": [1, 3, 5, 7],\n",
    "                \"gamma\": [0, 0.1, 0.2, 0.3, 0.4],\n",
    "            },\n",
    "        }\n",
    "\n",
    "    @log_method_call\n",
    "    @method_timer\n",
    "    def train_models(self, random_state: int = 42) -> None:\n",
    "        \"\"\"\n",
    "        Train models with hyperparameter optimization using RandomizedSearchCV.\n",
    "\n",
    "        Args:\n",
    "            random_state (int): Random state for reproducibility\n",
    "        \"\"\"\n",
    "        try:\n",
    "            param_distributions = self.get_param_distributions()\n",
    "            self.models = {}\n",
    "            self.best_params = {}\n",
    "            \n",
    "            for name, base_model in self.initialize_models(random_state).items():\n",
    "                self.logger.info(f\"Training {name} with hyperparameter optimization\")\n",
    "                \n",
    "                search = RandomizedSearchCV(\n",
    "                    estimator=base_model,\n",
    "                    param_distributions=param_distributions[name],\n",
    "                    n_iter=20,\n",
    "                    cv=5,\n",
    "                    scoring=\"roc_auc\",\n",
    "                    n_jobs=-1,\n",
    "                    random_state=random_state,\n",
    "                    verbose=1,\n",
    "                )\n",
    "                \n",
    "                search.fit(self.X_train, self.y_train)\n",
    "                \n",
    "                self.models[name] = search.best_estimator_\n",
    "                self.best_params[name] = search.best_params_\n",
    "                self.logger.info(f\"Best parameters for {name}: {search.best_params_}\")\n",
    "                self.logger.info(f\"Best cross-validation score for {name}: {search.best_score_:.4f}\")\n",
    "                self.results[name] = self._evaluate_model(search.best_estimator_)\n",
    "            return None\n",
    "        except Exception as e:\n",
    "            self.logger.error(\"Error during model training and optimization\")\n",
    "            raise ModelTrainingError(e) from e\n",
    "\n",
    "    @log_method_call\n",
    "    @method_timer\n",
    "    def _evaluate_model(self, model: BaseEstimator) -> ModelEvaluationResult:\n",
    "        \"\"\"\n",
    "        Evaluate a trained model and return performance metrics.\n",
    "\n",
    "        Args:\n",
    "            model (BaseEstimator): Trained sklearn-compatible model\n",
    "\n",
    "        Returns:\n",
    "            Dict[str, Any]: Dictionary containing evaluation metrics\n",
    "        \"\"\"\n",
    "        try:\n",
    "            y_pred = model.predict(self.X_test)\n",
    "            y_pred_proba = model.predict_proba(self.X_test)[:, 1]\n",
    "\n",
    "            return {\n",
    "                \"classification_report\": classification_report(self.y_test, y_pred),\n",
    "                \"roc_auc_score\": roc_auc_score(self.y_test, y_pred_proba),\n",
    "                \"y_pred\": y_pred,\n",
    "                \"y_pred_proba\": y_pred_proba,\n",
    "            }\n",
    "        except Exception as e:\n",
    "            self.logger.error(\"Error evaluating model\")\n",
    "            raise ModelEvaluationError(e) from e\n",
    "\n",
    "\n",
    "    @log_method_call\n",
    "    def print_results(self):\n",
    "        \"\"\"\n",
    "        Print optimization results and model performance metrics.\n",
    "        \"\"\"\n",
    "        for name in self.models:\n",
    "            print(f\"\\n--- {name} Results ---\")\n",
    "            print(f\"Best Parameters: {self.best_params[name]}\")\n",
    "            print(\"\\nTest Set Performance:\")\n",
    "            print(self.results[name][\"classification_report\"])\n",
    "            print(f\"ROC AUC Score: {self.results[name]['roc_auc_score']:.4f}\")\n",
    "\n",
    "    @log_method_call\n",
    "    def plot_calibration_curves(self):\n",
    "        \"\"\"\n",
    "        Plot calibration curves for all trained models.\n",
    "        \"\"\"\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        for name, model in self.models.items():\n",
    "            CalibrationDisplay.from_estimator(\n",
    "                model, self.X_test, self.y_test, n_bins=10, name=name\n",
    "            )\n",
    "        plt.title(\"Calibration Curves\")\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "    @log_method_call\n",
    "    def plot_lift_curves(self):\n",
    "        \"\"\"\n",
    "        Plot lift curves for all trained models.\n",
    "        \"\"\"\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        for name, result in self.results.items():\n",
    "            self._plot_single_lift_curve(self.y_test, result[\"y_pred_proba\"], name)\n",
    "        plt.title(\"Lift Curves\")\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "    @staticmethod\n",
    "    def _plot_single_lift_curve(\n",
    "        y_true: np.ndarray, y_scores: np.ndarray, model_name: str\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Plot a lift curve for a single model.\n",
    "\n",
    "        Args:\n",
    "            y_true (np.ndarray): True labels\n",
    "            y_scores (np.ndarray): Predicted probabilities\n",
    "            model_name (str): Name of the model for the legend\n",
    "        \"\"\"\n",
    "        data = pd.DataFrame({\"y_true\": y_true, \"y_scores\": y_scores})\n",
    "        data.sort_values(\"y_scores\", ascending=False, inplace=True)\n",
    "\n",
    "        data[\"cum_true\"] = data[\"y_true\"].cumsum()\n",
    "        data[\"lift\"] = data[\"cum_true\"] / data[\"y_true\"].sum()\n",
    "        data[\"baseline\"] = np.arange(1, len(data) + 1) / len(data)\n",
    "\n",
    "        plt.plot(data[\"lift\"].values, label=f\"{model_name} Lift Curve\")\n",
    "        plt.plot(data[\"baseline\"].values, label=\"Baseline\", linestyle=\"--\")\n",
    "        plt.xlabel(\"Percentage of Sample\")\n",
    "        plt.ylabel(\"Percentage of Positive Outcomes\")\n",
    "\n",
    "    @log_method_call\n",
    "    def get_best_model(self, metric: str = \"roc_auc_score\") -> str:\n",
    "        \"\"\"\n",
    "        Get the name of the best performing model based on a specified metric.\n",
    "\n",
    "        Args:\n",
    "            metric (str): Metric to use for comparison (default is 'roc_auc_score')\n",
    "\n",
    "        Returns:\n",
    "            str: Name of the best performing model\n",
    "        \"\"\"\n",
    "        return max(self.results, key=lambda x: self.results[x][metric])\n",
    "\n",
    "    @log_method_call\n",
    "    def save_models(self, directory: str = \"models\") -> None:\n",
    "        \"\"\"\n",
    "        Saves each model in the `models` dictionary to the specified directory using `joblib`.\n",
    "        The models are saved with filenames in the format `{model_name}_model.joblib`.\n",
    "\n",
    "        Args:\n",
    "            directory (str, optional): The directory where models will be saved. Defaults to `\"models\"`.\n",
    "\n",
    "        Returns:\n",
    "            None\n",
    "        \"\"\"\n",
    "        directory_path = Path(directory)\n",
    "        directory_path.mkdir(parents=True, exist_ok=True)\n",
    "        for name, model in self.models.items():\n",
    "            filename = directory_path / f\"{name}_model.joblib\"\n",
    "            joblib.dump(model, filename)\n",
    "            self.logger.info(f\"Model {name} saved to {filename}\")\n",
    "        return None\n",
    "\n",
    "    @log_method_call\n",
    "    def load_models(self, directory: str = \"models\") -> None:\n",
    "        \"\"\"\n",
    "        Loads models from the specified directory into the `models` dictionary.\n",
    "        It expects model files with filenames in the format `{model_name}_model.joblib`.\n",
    "\n",
    "        Args:\n",
    "            directory (str, optional): The directory from which models will be loaded. Defaults to `\"models\"`.\n",
    "\n",
    "        Returns:\n",
    "            None\n",
    "        \"\"\"\n",
    "        directory_path = Path(directory)\n",
    "        self.models = {}\n",
    "        for filepath in directory_path.glob(\"*_model.joblib\"):\n",
    "            name = filepath.stem.replace(\"_model\", \"\")\n",
    "            model = joblib.load(filepath)\n",
    "            self.models[name] = model\n",
    "            self.logger.info(f\"Model {name} loaded from {filepath}\")\n",
    "        return None\n",
    "\n",
    "    @log_method_call\n",
    "    def run_all(self) -> None:\n",
    "        \"\"\"\n",
    "        Run all steps: train models with optimization, print results, and plot curves.\n",
    "        \"\"\"\n",
    "        self.train_models()\n",
    "        self.print_results()\n",
    "        self.plot_calibration_curves()\n",
    "        self.plot_lift_curves()\n",
    "        self.save_models()\n",
    "        best_model = self.get_best_model()\n",
    "        print(f\"\\nBest model based on ROC AUC score: {best_model}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with BlockTimer(\"Model training and evaluation\"):\n",
    "    trainer = ModelTrainer(X_train, X_test, y_train, y_test)\n",
    "    trainer.train_models()\n",
    "    trainer.print_results()\n",
    "    trainer.plot_calibration_curves()\n",
    "    trainer.plot_lift_curves()\n",
    "    trainer.save_models()\n",
    "\n",
    "    best_model = trainer.get_best_model()\n",
    "    print(f\"\\nBest performing model: {best_model}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelCalibrationTrainer(BaseTrainer):\n",
    "    \"\"\"\n",
    "    A class to train, calibrate, and evaluate models with a focus on profit optimization.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        X_train: pd.DataFrame,\n",
    "        X_test: pd.DataFrame,\n",
    "        y_train: pd.Series,\n",
    "        y_test: pd.Series,\n",
    "        profits: pd.Series,\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Initialize the ModelCalibrationTrainer with training and testing data.\n",
    "\n",
    "        Args:\n",
    "            X_train (pd.DataFrame): Training features\n",
    "            X_test (pd.DataFrame): Testing features\n",
    "            y_train (pd.Series): Training labels\n",
    "            y_test (pd.Series): Testing labels\n",
    "            profits (pd.Series): Profits for each sample\n",
    "        \"\"\"\n",
    "        self.X_train = X_train\n",
    "        self.X_test = X_test\n",
    "        self.y_train = y_train\n",
    "        self.y_test = y_test\n",
    "        self.profits = profits\n",
    "        self.models = {}\n",
    "        self.calibrated_models = {}\n",
    "        self.results = {}\n",
    "        self.logger = logging.getLogger(self.__class__.__name__)\n",
    "        return None\n",
    "\n",
    "    @staticmethod\n",
    "    def get_param_distributions() -> Dict[str, Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        Define hyperparameter search spaces for each model.\n",
    "\n",
    "        Returns:\n",
    "            Dict[str, Dict[str, Any]]: Dictionary of parameter distributions for each model\n",
    "        \"\"\"\n",
    "        return {\n",
    "            \"RandomForest\": {\n",
    "                \"n_estimators\": [100, 200, 300, 400, 500],\n",
    "                \"max_depth\": [None, 10, 20, 30, 40, 50],\n",
    "                \"min_samples_split\": [2, 5, 10],\n",
    "                \"min_samples_leaf\": [1, 2, 4],\n",
    "                \"max_features\": [\"sqrt\", \"log2\", None],\n",
    "                \"class_weight\": [\"balanced\", \"balanced_subsample\", None],\n",
    "            },\n",
    "            \"XGBoost\": {\n",
    "                \"n_estimators\": [100, 200, 300, 400, 500],\n",
    "                \"max_depth\": [3, 4, 5, 6, 7, 8],\n",
    "                \"learning_rate\": [0.01, 0.05, 0.1, 0.15, 0.2],\n",
    "                \"subsample\": [0.6, 0.7, 0.8, 0.9, 1.0],\n",
    "                \"colsample_bytree\": [0.6, 0.7, 0.8, 0.9, 1.0],\n",
    "                \"min_child_weight\": [1, 3, 5, 7],\n",
    "                \"gamma\": [0, 0.1, 0.2, 0.3, 0.4],\n",
    "            },\n",
    "        }\n",
    "\n",
    "    @log_method_call\n",
    "    @method_timer\n",
    "    def train_models(self, random_state: int = 42) -> None:\n",
    "        \"\"\"\n",
    "        Train models with hyperparameter optimization using RandomizedSearchCV.\n",
    "\n",
    "        Args:\n",
    "            random_state (int): Random state for reproducibility\n",
    "        \"\"\"\n",
    "        try:\n",
    "            param_distributions = self.get_param_distributions()\n",
    "            self.models = {}\n",
    "            self.best_params = {}\n",
    "            \n",
    "            for name, base_model in ModelTrainer.initialize_models(random_state).items():\n",
    "                self.logger.info(f\"Training {name} with hyperparameter optimization\")\n",
    "                \n",
    "                search = RandomizedSearchCV(\n",
    "                    estimator=base_model,\n",
    "                    param_distributions=param_distributions[name],\n",
    "                    n_iter=20,\n",
    "                    cv=5,\n",
    "                    scoring=\"roc_auc\",\n",
    "                    n_jobs=-1,\n",
    "                    random_state=random_state,\n",
    "                    verbose=1,\n",
    "                )\n",
    "                \n",
    "                search.fit(self.X_train, self.y_train)\n",
    "                \n",
    "                self.models[name] = search.best_estimator_\n",
    "                self.best_params[name] = search.best_params_\n",
    "                self.logger.info(f\"Best parameters for {name}: {search.best_params_}\")\n",
    "                self.logger.info(f\"Best cross-validation score for {name}: {search.best_score_:.4f}\")\n",
    "            return None\n",
    "        except Exception as e:\n",
    "            self.logger.error(\"Error training models\")\n",
    "            raise ModelTrainingError(e) from e\n",
    "\n",
    "    @staticmethod\n",
    "    def calibrate_model(\n",
    "        model: BaseEstimator, X_train: pd.DataFrame, y_train: pd.Series, cv: int = 5\n",
    "    ) -> BaseEstimator:\n",
    "        \"\"\"\n",
    "        Calibrate a single model using CalibratedClassifierCV.\n",
    "\n",
    "        Args:\n",
    "            model (BaseEstimator): The model to calibrate\n",
    "            X_train (pd.DataFrame): Training features\n",
    "            y_train (pd.Series): Training labels\n",
    "            cv (int): Number of cross-validation folds for calibration\n",
    "\n",
    "        Returns:\n",
    "            BaseEstimator: Calibrated model\n",
    "        \"\"\"\n",
    "        calibrated_model = CalibratedClassifierCV(model, cv=cv, method=\"isotonic\")\n",
    "        calibrated_model.fit(X_train, y_train)\n",
    "        return calibrated_model\n",
    "\n",
    "    @log_method_call\n",
    "    @method_timer\n",
    "    def calibrate_models(self, cv: int = 5) -> None:\n",
    "        \"\"\"\n",
    "        Calibrate all trained models using CalibratedClassifierCV.\n",
    "\n",
    "        Args:\n",
    "            cv (int): Number of cross-validation folds for calibration\n",
    "        \"\"\"\n",
    "        for name, model in self.models.items():\n",
    "            calibrated_model = self.calibrate_model(\n",
    "                model, self.X_train, self.y_train, cv=cv\n",
    "            )\n",
    "            self.calibrated_models[name] = calibrated_model\n",
    "            self.results[name] = self._evaluate_model(calibrated_model)\n",
    "        return None\n",
    "\n",
    "    @log_method_call\n",
    "    @method_timer\n",
    "    def _evaluate_model(self, model: BaseEstimator) -> ModelEvaluationResult:\n",
    "        \"\"\"\n",
    "        Evaluate a calibrated model and return performance metrics.\n",
    "\n",
    "        Args:\n",
    "            model (BaseEstimator): Calibrated model\n",
    "\n",
    "        Returns:\n",
    "            Dict[str, Any]: Dictionary containing evaluation metrics\n",
    "        \"\"\"\n",
    "        try:\n",
    "            y_pred = model.predict(self.X_test)\n",
    "            y_pred_proba = model.predict_proba(self.X_test)[:, 1]\n",
    "            expected_profit = y_pred_proba * self.profits\n",
    "\n",
    "            return {\n",
    "                \"classification_report\": classification_report(self.y_test, y_pred),\n",
    "                \"roc_auc_score\": roc_auc_score(self.y_test, y_pred_proba),\n",
    "                \"brier_score\": brier_score_loss(self.y_test, y_pred_proba),\n",
    "                \"expected_profit\": expected_profit.sum(),\n",
    "                \"y_pred\": y_pred,\n",
    "                \"y_pred_proba\": y_pred_proba,\n",
    "            }\n",
    "        except Exception as e:\n",
    "            self.logger.error(\"Error evaluating model\")\n",
    "            raise ModelEvaluationError(e) from e\n",
    "\n",
    "    @log_method_call\n",
    "    def print_results(self) -> None:\n",
    "        \"\"\"\n",
    "        Print results for calibrated models.\n",
    "        \"\"\"\n",
    "        for name, result in self.results.items():\n",
    "            print(f\"\\n--- Calibrated {name} Results ---\")\n",
    "            print(f\"Best Parameters: {self.best_params[name]}\")\n",
    "            print(\"\\nTest Set Performance:\")\n",
    "            print(result[\"classification_report\"])\n",
    "            print(f\"ROC AUC Score: {result['roc_auc_score']:.4f}\")\n",
    "            print(f\"Brier Score: {result['brier_score']:.4f}\")\n",
    "            print(f\"Expected Profit: ${result['expected_profit']:.2f}\")\n",
    "\n",
    "    @log_method_call\n",
    "    def plot_lift_curves(self):\n",
    "        \"\"\"\n",
    "        Plot lift curves for all trained models.\n",
    "        \"\"\"\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        for name, result in self.results.items():\n",
    "            self._plot_single_lift_curve(self.y_test, result[\"y_pred_proba\"], name)\n",
    "        plt.title(\"Lift Curves\")\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "    @staticmethod\n",
    "    def _plot_single_lift_curve(\n",
    "        y_true: np.ndarray, y_scores: np.ndarray, model_name: str\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Plot a lift curve for a single model.\n",
    "\n",
    "        Args:\n",
    "            y_true (np.ndarray): True labels\n",
    "            y_scores (np.ndarray): Predicted probabilities\n",
    "            model_name (str): Name of the model for the legend\n",
    "        \"\"\"\n",
    "        data = pd.DataFrame({\"y_true\": y_true, \"y_scores\": y_scores})\n",
    "        data.sort_values(\"y_scores\", ascending=False, inplace=True)\n",
    "\n",
    "        data[\"cum_true\"] = data[\"y_true\"].cumsum()\n",
    "        data[\"lift\"] = data[\"cum_true\"] / data[\"y_true\"].sum()\n",
    "        data[\"baseline\"] = np.arange(1, len(data) + 1) / len(data)\n",
    "\n",
    "        plt.plot(data[\"lift\"].values, label=f\"{model_name} Lift Curve\")\n",
    "        plt.plot(data[\"baseline\"].values, label=\"Baseline\", linestyle=\"--\")\n",
    "        plt.xlabel(\"Percentage of Sample\")\n",
    "        plt.ylabel(\"Percentage of Positive Outcomes\")\n",
    "\n",
    "    @log_method_call\n",
    "    def plot_calibration_curves(self) -> None:\n",
    "        \"\"\"\n",
    "        Plot calibration curves for all calibrated models.\n",
    "        \"\"\"\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        for name, model in self.calibrated_models.items():\n",
    "            CalibrationDisplay.from_estimator(\n",
    "                model, self.X_test, self.y_test, n_bins=10, name=f\"Calibrated {name}\"\n",
    "            )\n",
    "        plt.title(\"Calibration Curves\")\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "    @log_method_call\n",
    "    def plot_profit_curves(self) -> None:\n",
    "        \"\"\"\n",
    "        Plot profit curves for all calibrated models.\n",
    "        \"\"\"\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        for name, result in self.results.items():\n",
    "            self._plot_single_profit_curve(\n",
    "                self.y_test, result[\"y_pred_proba\"], self.profits, name\n",
    "            )\n",
    "        plt.title(\"Profit Curves\")\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "        return None\n",
    "\n",
    "    @staticmethod\n",
    "    def _plot_single_profit_curve(\n",
    "        y_true: np.ndarray,\n",
    "        y_scores: np.ndarray,\n",
    "        profits: np.ndarray,\n",
    "        model_name: str,\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Plot a profit curve for a single model.\n",
    "\n",
    "        Args:\n",
    "            y_true (np.ndarray): True labels\n",
    "            y_scores (np.ndarray): Predicted probabilities\n",
    "            profits (np.ndarray): Profits for each sample\n",
    "            model_name (str): Name of the model for the legend\n",
    "        \"\"\"\n",
    "        data = pd.DataFrame(\n",
    "            {\"y_true\": y_true, \"y_scores\": y_scores, \"profit\": profits}\n",
    "        )\n",
    "        data.sort_values(\"y_scores\", ascending=False, inplace=True)\n",
    "\n",
    "        data[\"cum_profit\"] = (data[\"y_true\"] * data[\"profit\"]).cumsum()\n",
    "        data[\"percentage\"] = np.arange(1, len(data) + 1) / len(data)\n",
    "        total_possible_profit = (data[\"y_true\"] * data[\"profit\"]).sum()\n",
    "\n",
    "        plt.plot(\n",
    "            data[\"percentage\"],\n",
    "            data[\"cum_profit\"] / total_possible_profit,\n",
    "            label=f\"{model_name} Profit Curve\",\n",
    "        )\n",
    "\n",
    "        plt.plot([0, 1], [0, 1], linestyle=\"--\", color=\"gray\", label=\"Baseline\")\n",
    "\n",
    "        plt.xlabel(\"Percentage of Sample\")\n",
    "        plt.ylabel(\"Percentage of Total Possible Profit\")\n",
    "        plt.title(\"Cumulative Profit Curve\")\n",
    "        return None\n",
    "\n",
    "    @log_method_call\n",
    "    def get_best_model(self, metric: str = \"expected_profit\") -> str:\n",
    "        \"\"\"\n",
    "        Get the name of the best performing calibrated model based on a specified metric.\n",
    "\n",
    "        Args:\n",
    "            metric (str): Metric to use for comparison (default is 'expected_profit')\n",
    "\n",
    "        Returns:\n",
    "            str: Name of the best performing calibrated model\n",
    "        \"\"\"\n",
    "        return max(self.results, key=lambda x: self.results[x][metric])\n",
    "\n",
    "    @log_method_call\n",
    "    def save_models(self, directory: str = \"models\") -> None:\n",
    "        \"\"\"\n",
    "        Saves each model in the `models` dictionary to the specified directory using `joblib`.\n",
    "        The models are saved with filenames in the format `{model_name}_model.joblib`.\n",
    "\n",
    "        Args:\n",
    "            directory (str, optional): The directory where models will be saved. Defaults to `\"models\"`.\n",
    "\n",
    "        Returns:\n",
    "            None\n",
    "        \"\"\"\n",
    "        directory_path = Path(directory)\n",
    "        directory_path.mkdir(parents=True, exist_ok=True)\n",
    "        for name, model in self.models.items():\n",
    "            filename = directory_path / f\"{name}_model.joblib\"\n",
    "            joblib.dump(model, filename)\n",
    "            self.logger.info(f\"Model {name} saved to {filename}\")\n",
    "        return None\n",
    "\n",
    "    @log_method_call\n",
    "    def load_models(self, directory: str = \"models\") -> None:\n",
    "        \"\"\"\n",
    "        Loads models from the specified directory into the `models` dictionary.\n",
    "        It expects model files with filenames in the format `{model_name}_model.joblib`.\n",
    "\n",
    "        Args:\n",
    "            directory (str, optional): The directory from which models will be loaded. Defaults to `\"models\"`.\n",
    "\n",
    "        Returns:\n",
    "            None\n",
    "        \"\"\"\n",
    "        directory_path = Path(directory)\n",
    "        self.models = {}\n",
    "        for filepath in directory_path.glob(\"*_model.joblib\"):\n",
    "            name = filepath.stem.replace(\"_model\", \"\")\n",
    "            model = joblib.load(filepath)\n",
    "            self.models[name] = model\n",
    "            self.logger.info(f\"Model {name} loaded from {filepath}\")\n",
    "        return None\n",
    "\n",
    "    @log_method_call\n",
    "    def run_all(self) -> None:\n",
    "        \"\"\"\n",
    "        Run all steps: train models, calibrate models, print results, and plot curves.\n",
    "        \"\"\"\n",
    "        self.train_models()\n",
    "        self.calibrate_models()\n",
    "        self.print_results()\n",
    "        self.plot_calibration_curves()\n",
    "        self.plot_lift_curves()\n",
    "        self.plot_profit_curves()\n",
    "        self.save_models()\n",
    "        best_model = self.get_best_model()\n",
    "        print(f\"\\nBest calibrated model based on expected profit: {best_model}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with BlockTimer(\"Model training, calibration and evaluation\"):\n",
    "    profits_test = data.loc[X_test.index, \"Profit\"]\n",
    "    calibration_trainer = ModelCalibrationTrainer(\n",
    "        X_train, X_test, y_train, y_test, profits_test\n",
    "    )\n",
    "    calibration_trainer.train_models()\n",
    "    calibration_trainer.calibrate_models()\n",
    "    calibration_trainer.print_results()\n",
    "    calibration_trainer.plot_calibration_curves()\n",
    "    calibration_trainer.plot_lift_curves()\n",
    "    calibration_trainer.plot_profit_curves()\n",
    "    calibration_trainer.save_models()\n",
    "\n",
    "    best_model = calibration_trainer.get_best_model()\n",
    "    print(f\"\\nBest calibrated model based on expected profit: {best_model}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "4it533",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
